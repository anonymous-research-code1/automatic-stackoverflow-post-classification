{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install openai;\n",
    "!pip install anthropic;\n",
    "!pip install chromadb;\n",
    "!pip install transformers;\n",
    "!pip install huggingface_hub;\n",
    "!pip install --upgrade huggingface_hub;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.956Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import anthropic\n",
    "import openai\n",
    "import sklearn\n",
    "import torch\n",
    "import chromadb\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "token = #huggingface token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_key =  #your key\n",
    "openai.api_key = openai_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "\n",
    "\n",
    "def query_gpt_chat(api_key, messages, model=\"gpt-3.5-turbo-0125\", temperature=0.2,max_tokens =1024):\n",
    "    openai.api_key = api_key\n",
    "    client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens =max_tokens,\n",
    "        temperature = temperature\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "deepseek_key = #your deepseek key\n",
    "os.environ[\"deepseek_key\"] = deepseek_key\n",
    "def query_deepseek(api_key, messages, model=\"deepseek-ai/deepseek-r1\",temperature=0.2,max_tokens =1024):\n",
    "    \n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url= \"https://integrate.api.nvidia.com/v1\",\n",
    "        )\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens =max_tokens,\n",
    "        temperature = temperature,\n",
    "        stream=False\n",
    "    )\n",
    "    response_text = response.choices[0].message.content\n",
    "    return re.sub(r\"<think>.*?</think>\\n*\", \"\", response_text, flags=re.DOTALL).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ANTHROPIC_API_KEY= #your key\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "def query_claude(api_key, messages_system, messages_user, model=\"claude-3-5-sonnet-20241022\",temperature=0.2,max_tokens =1024):\n",
    "    client = anthropic.Anthropic(api_key=api_key)\n",
    "    message = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        system=messages_system,\n",
    "        messages=messages_user\n",
    "    )\n",
    "    return message.content[0].text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_llm(text,system=None,temperature=0.2):\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    sys_prompt = system \n",
    "    msg= [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    return query_gpt_chat(api_key, msg,temperature=temperature)\n",
    "\n",
    "    #deepseek\n",
    "    # api_key = os.getenv(\"deepseek_key\")\n",
    "    # return query_deepseek(api_key, msg) \n",
    "           \n",
    "    \n",
    "    #CLAUD\n",
    "    # api_key_antropic = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    # return query_claude( api_key=api_key_antropic, messages_system=sys_prompt, messages_user=[{\"role\": \"user\", \"content\": text}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding (encder) setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "\n",
    "\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "MASK_TOKEN = \"<mask>\"\n",
    "SEPARATOR_TOKEN = \"<sep>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "CLS_TOKEN = \"<cls>\"\n",
    "\n",
    "def prepare_tokenizer(tokenizer):\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.add_special_tokens({\"sep_token\": SEPARATOR_TOKEN})\n",
    "    tokenizer.add_special_tokens({\"cls_token\": CLS_TOKEN})\n",
    "    tokenizer.add_special_tokens({\"mask_token\": MASK_TOKEN})\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = prepare_tokenizer(tokenizer)\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts.tolist(), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512) \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs\n",
    "\n",
    "def save_embeddings(embeddings, file_path):\n",
    "    np.save(file_path, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Sample Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.966Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_nearest_samples(test_embedding, collection, n_neighbors=20):\n",
    "    query_results = collection.query(query_embeddings=[test_embedding], n_results=n_neighbors)\n",
    "    \n",
    "    documents = query_results['documents']\n",
    "    metadatas = query_results['metadatas']\n",
    "    \n",
    "    nearest_by_class = {}\n",
    "\n",
    "    for doc, meta, distance in zip(documents[0], metadatas[0], query_results['distances'][0]):\n",
    "        bug_type = meta['bug_type']\n",
    "        if bug_type not in nearest_by_class:\n",
    "            nearest_by_class[bug_type] = {'distance': distance, 'document': doc, 'metadata': meta}\n",
    "        elif distance < nearest_by_class[bug_type]['distance']:\n",
    "            nearest_by_class[bug_type] = {'distance': distance, 'document': doc, 'metadata': meta}\n",
    "    \n",
    "    nearest_documents = [entry['document'] for entry in nearest_by_class.values()]\n",
    "    nearest_metadatas = [entry['metadata'] for entry in nearest_by_class.values()]\n",
    "    return nearest_documents, nearest_metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_data_path.csv\")  # Replace with your actual train data path\n",
    "test_df = pd.read_csv(\"test_data_path.csv\")\n",
    "\n",
    "# use this function for summarizing training posts. Stage 2\n",
    "def summary_with_cot(post, answer, label, label_description):\n",
    "    prompt = f\"Here is the stackoverflow post below. \\n\\n <POST>{post}</POST>. \\n\\n <Answer> {answer} </Answer> \\n and <Class lable> {label} </Class label> and \\n <Lable Description> {label_description} </Lable Description>\"\n",
    "    system_prompt= \"You are an expert in summarization and reasoning. You are given a Stack Overflow post containing an <Answer> and its associated <Label> along with the <Label Description> presenting what it means. Your task is to: Concisely summarize the post, preserving all critical information. Explain why the post has been assigned the given label, providing a clear chain of thought and reasoning. Ensure the response is structured as a short paragraph that integrates the summary, the solution, and the reasoning seamlessly. Avoid phrases like `The summary is' and focus on providing a cohesive and informative output.\"\n",
    "    return run_llm(prompt, system_prompt,1.0)\n",
    "\n",
    "\n",
    "cls = 'bug_type'  # Replace with your actual class label column name. \n",
    "\n",
    "X_train= train_df['post']\n",
    "X_train_summary = train_df['summary_with_cot'] #from stage 2. Already saved to save computation\n",
    "y_train = train_df[cls]\n",
    "\n",
    "X_test = test_df['post']\n",
    "y_test = test_df[cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concept_distillation(post, label):\n",
    "    prompt = f\"Here is the stackoverflow post below. \\n\\n <POST>{post}</POST>. \\n\\n <Lable> {label} </label>\"\n",
    "    concept_distillation_prompt= \"You are an expert in summarization. You are given a Stack Overflow post with the <Label> of the <Post>. Now return the characteristic of the post in very short (1/2 sentences) by observing the label of the post. The return output may look like: 'ValueError: class_weight in Keras fails for multi-label binary classification'\"\n",
    "    return run_llm(prompt, concept_distillation_prompt,1.0)\n",
    "\n",
    "\n",
    "\n",
    "def consolidation(text):\n",
    "    prompt = f\"Here is the characteristics below. \\n\\n <Text>{text}</Text>.\"\n",
    "    consolidate_prompt = \"You are an expert in summarization. You are given some characteristics of a particular class. You have to observe the characteristics and return a summary of them by removing redundancy and preserving the uniqueness. It should not be very large nor small. Just return the summary. Do not start with `the summary is' or `summary:' type words.\"\n",
    "    return run_llm(prompt, consolidate_prompt,1.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept distillation and consolidation for training data (stage 1)\n",
    "\n",
    "result_dict = {}\n",
    "for bug_type, group in train_df.groupby(cls):\n",
    "    result_dict[bug_type] = [concept_distillation(row_text, bug_type) for row_text in group['text']]\n",
    "result_dict_summary = {}\n",
    "for class_ in result_dict.keys():\n",
    "    result_dict_summary[class_]=consolidation(\"\\n\".join(result_dict[class_]),class_)\n",
    "result_dict_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embeddings = get_embeddings(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing data in Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "client_chroma = chromadb.Client()\n",
    "collection_name = \"bug_db\"\n",
    "collection = client_chroma.get_or_create_collection(name=collection_name)\n",
    "import math\n",
    "\n",
    "\n",
    "for idx, (embedding, text, label,summary) in enumerate(zip(embeddings, X_train, y_train, X_train_summary)):\n",
    "\n",
    "    collection.add(\n",
    "        embeddings = [embedding.tolist()],\n",
    "        documents=text,\n",
    "        metadatas=[{'bug_type': label, 'summay':summary}],\n",
    "        ids=[f\"doc_{idx}\"]  \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cls_list = list(y_train.unique())\n",
    "cls_description = result_dict_summary #from stage 1\n",
    "system_prompt =  f\"\"\"You are a one-shot classifier that categorizes Stack Overflow posts. You have to return one value from this list only: {cls_list}. Based on the bug type of the test sample, return only ONE VALUE FROM THE LIST. The characteristics of each class are provided below:\"\"\"\n",
    "\n",
    "for class_name in cls_list:\n",
    "\n",
    "    system_prompt = system_prompt +\"\\n\"+class_name+\": \"+cls_description[class_name]\n",
    "    print(len(cls_description[class_name]))\n",
    "\n",
    "    \n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def classify_with_llm(test_sample, nearest_samples,system_prompt):\n",
    "    prompt = \"\"\n",
    "    for neighbor_text, neighbor_metadata in zip(nearest_samples[0], nearest_samples[1]):\n",
    "        neighbor_label = neighbor_metadata['bug_type']\n",
    "\n",
    "        neighbor_summary = neighbor_metadata['summay']\n",
    "        prompt += f\"Example Post: {neighbor_summary}. <class>: {neighbor_label}\\n\"\n",
    "\n",
    "        \n",
    "    prompt += f\"\\nTest Sample: {test_sample}. <class>?:\"\n",
    "    \n",
    "    \n",
    "    result = run_llm(prompt,system_prompt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def simple_summary(post):\n",
    "    system_prompt=\"You are given a Stack Overflow post. You have to summarize the post. You may illustrate the error in detail. Do not provide any solution to this problem. The overall output should be short. Just return the summary without extra words.\"\n",
    "    client_prompt = f\"<Post> {post} </Post>\"\n",
    "    return run_llm(client_prompt,system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# true_labels = y_test.tolist()\n",
    "predictions_bt = []\n",
    "not_found_count = 0\n",
    "not_founds = [ ]\n",
    "for i in range(len(X_test)):#\n",
    "    test_embedding = get_embeddings(pd.Series([X_test.iloc[i]]))[0]\n",
    "    nearest_documents, nearest_metadatas = find_nearest_samples(test_embedding, collection)\n",
    "    X_test_summary = simple_summary(X_test.iloc[i]]) #\n",
    "    classification = classify_with_llm(X_test_summary, [nearest_documents, nearest_metadatas],system_prompt)\n",
    "    \n",
    "    class_found = False\n",
    "    for class_name in y_train.unique():\n",
    "        if class_name.lower().strip() in classification.lower().strip():\n",
    "            predictions_bt.append(class_name)\n",
    "            print(f\"{i}, pred: {class_name}\")\n",
    "            class_found = True\n",
    "            break\n",
    "    \n",
    "    if not class_found:\n",
    "        print(f\"not found: {classification}\")\n",
    "        predictions_bt.append(classification)\n",
    "        not_founds.append(i)\n",
    "        not_found_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T20:34:22.972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#execute this code only if LLM fails to make a prediction from the class list\n",
    "\n",
    "not_founds_2=[]\n",
    "for i in not_founds:\n",
    "    test_embedding = get_embeddings(pd.Series([X_test.iloc[i]]))[0]\n",
    "    nearest_documents, nearest_metadatas = find_nearest_samples(test_embedding, collection)\n",
    "    classification = classify_with_llm(X_test_summary.iloc[i], [nearest_documents, nearest_metadatas],system_prompt)\n",
    "    \n",
    "    class_found = False\n",
    "    for class_name in y_test.unique():\n",
    "        if class_name.strip().lower() in classification.strip().lower():\n",
    "            predictions_bt[i] =  class_name\n",
    "            print(f\"{i}, class: {class_name}\\n\\n\")\n",
    "            class_found = True\n",
    "            break\n",
    "    \n",
    "    if not class_found:\n",
    "        print(f\"not found: {classification}\")\n",
    "        not_founds_2.append(i)\n",
    "        not_found_count += 1"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5980792,
     "sourceId": 10863342,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
