{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10863342,"sourceType":"datasetVersion","datasetId":5980792}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openai;\n!pip install anthropic;\n!pip install chromadb;\n!pip install transformers;\n!pip install huggingface_hub;\n!pip install --upgrade huggingface_hub;","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"execution_failed":"2025-03-14T20:34:22.946Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport anthropic\nimport openai\nimport sklearn\nimport torch\nimport chromadb\nimport json\nfrom collections import defaultdict","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\ntoken = #huggingface token","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import openai\nimport os\nfrom openai import OpenAI\n\nopenai_key =  #your key\nopenai.api_key = openai_key\nos.environ[\"OPENAI_API_KEY\"] = openai_key\n\n\ndef query_gpt_chat(api_key, messages, model=\"gpt-3.5-turbo-0125\", temperature=0.2,max_tokens =1024):\n    openai.api_key = api_key\n    client = OpenAI(\n    api_key=api_key,\n    )\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        max_tokens =max_tokens,\n        temperature = temperature\n    )\n    return response.choices[0].message.content","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\ndeepseek_key = #your deepseek key\nos.environ[\"deepseek_key\"] = deepseek_key\ndef query_deepseek(api_key, messages, model=\"deepseek-ai/deepseek-r1\",temperature=0.2,max_tokens =1024):\n    \n    client = OpenAI(\n        api_key=api_key,\n        base_url= \"https://integrate.api.nvidia.com/v1\",\n        )\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        max_tokens =max_tokens,\n        temperature = temperature,\n        stream=False\n    )\n    response_text = response.choices[0].message.content\n    return re.sub(r\"<think>.*?</think>\\n*\", \"\", response_text, flags=re.DOTALL).strip()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ANTHROPIC_API_KEY= #your key\nos.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n\ndef query_claude(api_key, messages_system, messages_user, model=\"claude-3-5-sonnet-20241022\",temperature=0.2,max_tokens =1024):\n    client = anthropic.Anthropic(api_key=api_key)\n    message = client.messages.create(\n        model=model,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        system=messages_system,\n        messages=messages_user\n    )\n    return message.content[0].text ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\ndef run_llm(text,system=None):\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    sys_prompt = system \n    msg= [\n        {\"role\": \"system\", \"content\": sys_prompt},\n        {\"role\": \"user\", \"content\": text}\n    ]\n    return query_gpt_chat(api_key, msg)\n\n    #deepseek\n    # api_key = os.getenv(\"deepseek_key\")\n    # return query_deepseek(api_key, msg) \n           \n    \n    #CLAUD\n    # api_key_antropic = os.getenv(\"ANTHROPIC_API_KEY\")\n    # return query_claude( api_key=api_key_antropic, messages_system=sys_prompt, messages_user=[{\"role\": \"user\", \"content\": text}])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom chromadb.config import Settings\nimport chromadb\n\n\nMODEL_NAME = \"microsoft/codebert-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\n\n\nMASK_TOKEN = \"<mask>\"\nSEPARATOR_TOKEN = \"<sep>\"\nPAD_TOKEN = \"<pad>\"\nCLS_TOKEN = \"<cls>\"\n\ndef prepare_tokenizer(tokenizer):\n    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n    tokenizer.add_special_tokens({\"sep_token\": SEPARATOR_TOKEN})\n    tokenizer.add_special_tokens({\"cls_token\": CLS_TOKEN})\n    tokenizer.add_special_tokens({\"mask_token\": MASK_TOKEN})\n    return tokenizer\n\ntokenizer = prepare_tokenizer(tokenizer)\ndef get_embeddings(texts):\n    inputs = tokenizer(texts.tolist(), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n    return embeddings.cpu().numpy()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_text(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512) \n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs\n\ndef save_embeddings(embeddings, file_path):\n    np.save(file_path, embeddings)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_nearest_samples(test_embedding, collection, n_neighbors=20):\n    query_results = collection.query(query_embeddings=[test_embedding], n_results=n_neighbors)\n    \n    documents = query_results['documents']\n    metadatas = query_results['metadatas']\n    \n    nearest_by_class = {}\n\n    for doc, meta, distance in zip(documents[0], metadatas[0], query_results['distances'][0]):\n        bug_type = meta['bug_type']\n        if bug_type not in nearest_by_class:\n            nearest_by_class[bug_type] = {'distance': distance, 'document': doc, 'metadata': meta}\n        elif distance < nearest_by_class[bug_type]['distance']:\n            nearest_by_class[bug_type] = {'distance': distance, 'document': doc, 'metadata': meta}\n    \n    nearest_documents = [entry['document'] for entry in nearest_by_class.values()]\n    nearest_metadatas = [entry['metadata'] for entry in nearest_by_class.values()]\n    return nearest_documents, nearest_metadatas","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train= train_df['post']\nX_train_summary = summary_with_reasoning #from stage 1\ny_train = train_df[cls]\n\nX_test = test_df['post']\ny_test = test_df[cls]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings = get_embeddings(X_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"client_chroma = chromadb.Client()\ncollection_name = \"bug_db2\"\ncollection = client_chroma.get_or_create_collection(name=collection_name)\nimport math\n\n\nfor idx, (embedding, text, label,summary) in enumerate(zip(embeddings, X_train, y_train, X_train_summary)):\n\n    collection.add(\n        embeddings = [embedding.tolist()],\n        documents=text,\n        metadatas=[{'bug_type': label, 'summay':summary}],\n        ids=[f\"doc_{idx}\"]  \n    )\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cls_list = list(y_train.unique())\ncls_description = bug_type_character #from stage 1\nsystem_prompt =  f\"\"\"You are a one-shot classifier that categorizes Stack Overflow posts. You have to return one value from this list only: {cls_list}. Based on the bug type of the test sample, return only ONE VALUE FROM THE LIST. The characteristics of each class are provided below:\"\"\"\n\nfor class_name in cls_list:\n\n    system_prompt = system_prompt +\"\\n\"+class_name+\": \"+cls_description[class_name]\n    print(len(cls_description[class_name]))\n\n    \nprint(system_prompt)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def classify_with_llm(test_sample, nearest_samples,system_prompt):\n    prompt = \"\"\n    for neighbor_text, neighbor_metadata in zip(nearest_samples[0], nearest_samples[1]):\n        neighbor_label = neighbor_metadata['bug_type']\n\n        neighbor_summary = neighbor_metadata['summay']\n        prompt += f\"Example Post: {neighbor_summary}. <class>: {neighbor_label}\\n\"\n\n        \n    prompt += f\"\\nTest Sample: {test_sample}. <class>?:\"\n    \n    \n    result = run_llm(prompt,system_prompt)\n    return result","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def simple_summary(post):\n    system_prompt=\"You are given a Stack Overflow post. You have to summarize the post. You may illustrate the error in detail. Do not provide any solution to this problem. The overall output should be short. Just return the summary without extra words.\"\n    client_prompt = f\"<Post> {post} </Post>\"\n    return run_llm(client_prompt,system_prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, f1_score, matthews_corrcoef, confusion_matrix\nimport numpy as np\n\n# true_labels = y_test.tolist()\npredictions_bt = []\nnot_found_count = 0\nnot_founds = [ ]\nfor i in range(len(X_test)):#\n    test_embedding = get_embeddings(pd.Series([X_test.iloc[i]]))[0]\n    nearest_documents, nearest_metadatas = find_nearest_samples(test_embedding, collection)\n    X_test_summary = simple_summary(X_test.iloc[i]]) #\n    classification = classify_with_llm(X_test_summary, [nearest_documents, nearest_metadatas],system_prompt)\n    \n    class_found = False\n    for class_name in y_train.unique():\n        if class_name.lower().strip() in classification.lower().strip():\n            predictions_bt.append(class_name)\n            print(f\"{i}, pred: {class_name}\")\n            class_found = True\n            break\n    \n    if not class_found:\n        print(f\"not found: {classification}\")\n        predictions_bt.append(classification)\n        not_founds.append(i)\n        not_found_count += 1","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#execute this code only if LLM fails to make a prediction from the class list\n\nnot_founds_2=[]\nfor i in not_founds:\n    test_embedding = get_embeddings(pd.Series([X_test.iloc[i]]))[0]\n    nearest_documents, nearest_metadatas = find_nearest_samples(test_embedding, collection)\n    classification = classify_with_llm(X_test_summary.iloc[i], [nearest_documents, nearest_metadatas],system_prompt)\n    \n    class_found = False\n    for class_name in y_test.unique():\n        if class_name.strip().lower() in classification.strip().lower():\n            predictions_rc[i] =  class_name\n            print(f\"{i}, class: {class_name}\\n\\n\")\n            class_found = True\n            break\n    \n    if not class_found:\n        print(f\"not found: {classification}\")\n        not_founds_2.append(i)\n        not_found_count += 1","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-14T20:34:22.972Z"}},"outputs":[],"execution_count":null}]}